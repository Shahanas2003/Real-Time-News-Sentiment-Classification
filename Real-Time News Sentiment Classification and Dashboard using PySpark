{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbjiQn66KsWGHyu8lFXAhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahanas2003/Real-Time-News-Sentiment-Classification/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYN_dj4vKpBx",
        "outputId": "e17e3719-06e0-428b-96e2-f095152b0c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching news: 422\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# NewsData.io API Configuration\n",
        "# -----------------------------\n",
        "API_KEY = \"pub_7c7f72f816dc47c28a889f0c0a5b371f\"\n",
        "BASE_URL = \"https://newsdata.io/api/1/news\"\n",
        "\n",
        "# Function to fetch news\n",
        "def fetch_news(api_key, language=\"en\", country=\"us\", category=\"technology\", page=1):\n",
        "    params = {\n",
        "        \"apikey\": api_key,\n",
        "        \"language\": language,\n",
        "        \"country\": country,\n",
        "        \"category\": category,\n",
        "        \"page\": page\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"results\", [])\n",
        "    else:\n",
        "        print(f\"Error fetching news: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch news\n",
        "# -----------------------------\n",
        "articles = fetch_news(API_KEY)\n",
        "\n",
        "# -----------------------------\n",
        "# Display news\n",
        "# -----------------------------\n",
        "for idx, article in enumerate(articles, start=1):\n",
        "    title = article.get(\"title\")\n",
        "    description = article.get(\"description\")\n",
        "    link = article.get(\"link\")\n",
        "    pub_date = article.get(\"pubDate\")\n",
        "    source_id = article.get(\"source_id\")\n",
        "\n",
        "    print(f\"{idx}. {title}\")\n",
        "    print(f\"   Description: {description}\")\n",
        "    print(f\"   Link: {link}\")\n",
        "    print(f\"   Published: {pub_date}\")\n",
        "    print(f\"   Source: {source_id}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# -----------------------------\n",
        "# NewsData.io API Configuration\n",
        "# -----------------------------\n",
        "API_KEY = \"pub_7c7f72f816dc47c28a889f0c0a5b371f\"\n",
        "BASE_URL = \"https://newsdata.io/api/1/news\"\n",
        "\n",
        "# Allowed categories\n",
        "VALID_CATEGORIES = [\"business\", \"entertainment\", \"environment\", \"food\", \"health\",\n",
        "                    \"politics\", \"science\", \"sports\", \"technology\", \"top\"]\n",
        "\n",
        "def fetch_news(api_key, language=\"en\", category=\"top\", page=1):\n",
        "    if category not in VALID_CATEGORIES:\n",
        "        raise ValueError(f\"Invalid category '{category}'. Valid options: {VALID_CATEGORIES}\")\n",
        "\n",
        "    params = {\n",
        "        \"apikey\": api_key,\n",
        "        \"language\": language,\n",
        "        \"category\": category,\n",
        "        \"page\": page\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"results\", [])\n",
        "    else:\n",
        "        print(f\"Error fetching news: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch global top news\n",
        "# -----------------------------\n",
        "articles = fetch_news(API_KEY, category=\"top\")\n",
        "\n",
        "# -----------------------------\n",
        "# Display fetched news\n",
        "# -----------------------------\n",
        "if articles:\n",
        "    for idx, article in enumerate(articles, start=1):\n",
        "        print(f\"{idx}. {article.get('title')}\")\n",
        "        print(f\"   Description: {article.get('description')}\")\n",
        "        print(f\"   Link: {article.get('link')}\")\n",
        "        print(f\"   Published: {article.get('pubDate')}\")\n",
        "        print(f\"   Source: {article.get('source_id')}\")\n",
        "        print(\"-\" * 80)\n",
        "else:\n",
        "    print(\"No articles fetched.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_LKz7E0L-fG",
        "outputId": "d4c609f3-3a48-4018-b44e-a5e2184fe2d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching news: 422, {\"status\":\"error\",\"results\":{\"message\":\"The provided value for the next page is invalid. For more information on pagination, please refer to the Newsdata documentation at: https://newsdata.io/documentation/#pagination\",\"code\":\"UnsupportedFilter\"}}\n",
            "\n",
            "No articles fetched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# -----------------------------\n",
        "# NewsData.io API Configuration\n",
        "# -----------------------------\n",
        "API_KEY = \"pub_7c7f72f816dc47c28a889f0c0a5b371f\"\n",
        "BASE_URL = \"https://newsdata.io/api/1/news\"\n",
        "\n",
        "# Allowed categories\n",
        "VALID_CATEGORIES = [\"business\", \"entertainment\", \"environment\", \"food\", \"health\",\n",
        "                    \"politics\", \"science\", \"sports\", \"technology\", \"top\"]\n",
        "\n",
        "def fetch_news(api_key, language=\"en\", category=\"top\"):\n",
        "    if category not in VALID_CATEGORIES:\n",
        "        raise ValueError(f\"Invalid category '{category}'. Valid options: {VALID_CATEGORIES}\")\n",
        "\n",
        "    params = {\n",
        "        \"apikey\": api_key,\n",
        "        \"language\": language,\n",
        "        \"category\": category\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"results\", [])\n",
        "    else:\n",
        "        print(f\"Error fetching news: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch global top news\n",
        "# -----------------------------\n",
        "articles = fetch_news(API_KEY, category=\"top\")\n",
        "\n",
        "# -----------------------------\n",
        "# Display fetched news\n",
        "# -----------------------------\n",
        "if articles:\n",
        "    for idx, article in enumerate(articles, start=1):\n",
        "        print(f\"{idx}. {article.get('title')}\")\n",
        "        print(f\"   Description: {article.get('description')}\")\n",
        "        print(f\"   Link: {article.get('link')}\")\n",
        "        print(f\"   Published: {article.get('pubDate')}\")\n",
        "        print(f\"   Source: {article.get('source_id')}\")\n",
        "        print(\"-\" * 80)\n",
        "else:\n",
        "    print(\"No articles fetched.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu2a9kT7MEqW",
        "outputId": "e78ff700-4c8a-4112-cf28-91362fce6716"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Mining pit collapses in Zamfara, many feared dead\n",
            "   Description: A tragic mining accident has rocked Zamfara State, as a pit collapse at the Kadauri mining site in Maru Local Government Area is feared to have claimed numerous lives. TheThe post Mining pit collapses in Zamfara, many feared dead appeared first on National Accord Newspaper.\n",
            "   Link: https://www.nationalaccordnewspaper.com/mining-pit-collapses-in-zamfara-many-feared-dead/\n",
            "   Published: 2025-09-27 19:17:50\n",
            "   Source: thenationonlineng\n",
            "--------------------------------------------------------------------------------\n",
            "2. Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally in Tamil Nadu\n",
            "   Description: The organisers had announced that TVK president Vijay would speak at 12 p.m.; the crowd began to assemble from 9 a.m., but he did not arrive at the scheduled time.\n",
            "   Link: https://www.thehindu.com/news/national/tamil-nadu/karur-stampede-anatomy-of-the-tragedy-at-vijays-tvk-rally-in-tamil-nadu/article70103198.ece\n",
            "   Published: 2025-09-27 19:17:49\n",
            "   Source: thehindu\n",
            "--------------------------------------------------------------------------------\n",
            "3. WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans and asks for Ryder Cup respect\n",
            "   Description: Rory McIlroy hit back at American supporters after being subjected to heckling throughout his foursomes appearance on the second day of the Ryder Cup.\n",
            "   Link: https://www.newsletter.co.uk/sport/golf/watch-shut-the-f-up-rory-mcilroy-aims-expletive-at-american-fans-and-asks-for-ryder-cup-respect-5336683\n",
            "   Published: 2025-09-27 19:17:46\n",
            "   Source: newsletter\n",
            "--------------------------------------------------------------------------------\n",
            "4. Beware of downtown Boise traffic congestion after BSU football game\n",
            "   Description: The view from Boise Depot shows Capitol Boulevard, downtown Boise and the state Capitol.\n",
            "   Link: https://www.idahostatesman.com/news/local/traffic/article312286431.html\n",
            "   Published: 2025-09-27 19:17:46\n",
            "   Source: idahostatesman\n",
            "--------------------------------------------------------------------------------\n",
            "5. Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1\n",
            "   Description: Tarik Skubal should know soon will whether his services will be needed in the regular-season finale.\n",
            "   Link: https://www.mlive.com/tigers/2025/09/tigers-ace-tarik-skubal-prepares-for-a-big-start-be-it-sunday-or-game-1.html\n",
            "   Published: 2025-09-27 19:17:44\n",
            "   Source: mlive\n",
            "--------------------------------------------------------------------------------\n",
            "6. AP News Summary at 3:17 p.m. EDT\n",
            "   Description: Trump says he will send troops to Portland, Oregon, in latest deployment to US cities\n",
            "   Link: https://www.eagletribune.com/region/ap-news-summary-at-3-17-p-m-edt/article_29e54e93-c7e8-50cf-82fa-ce9977a0cf8f.html\n",
            "   Published: 2025-09-27 19:17:41\n",
            "   Source: eagletribune\n",
            "--------------------------------------------------------------------------------\n",
            "7. AP News Summary at 3:17 p.m. EDT\n",
            "   Description: Trump says he will send troops to Portland, Oregon, in latest deployment to US cities\n",
            "   Link: https://www.washtimesherald.com/news/national_news/ap-news-summary-at-3-17-p-m-edt/article_15b74817-dc09-540e-9646-bbb9cc86b9f1.html\n",
            "   Published: 2025-09-27 19:17:41\n",
            "   Source: washtimesherald\n",
            "--------------------------------------------------------------------------------\n",
            "8. New Tourism Projects A Rs 15,000 cr Scam: Harish\n",
            "   Description: In a statement, Harish Rao said land worth lakhs of crores of rupees was being readied to be handed over to a few chosen by Chief Minister A. Revanth Reddy in the guise of tourism projects in the state.\n",
            "   Link: https://www.deccanchronicle.com/southern-states/telangana/new-tourism-projects-a-rs-15000-cr-scam-harish-1906660\n",
            "   Published: 2025-09-27 19:17:41\n",
            "   Source: andhrabhoomi\n",
            "--------------------------------------------------------------------------------\n",
            "9. AP News Summary at 3:17 p.m. EDT\n",
            "   Description: Trump says he will send troops to Portland, Oregon, in latest deployment to US cities\n",
            "   Link: https://www.newsitem.com/ap/national/ap-news-summary-at-3-17-p-m-edt/article_c0712e9e-966d-5a8e-b2fe-fe0e084f5bb1.html\n",
            "   Published: 2025-09-27 19:17:41\n",
            "   Source: newsitem\n",
            "--------------------------------------------------------------------------------\n",
            "10. AP News Summary at 3:17 p.m. EDT\n",
            "   Description: Trump says he will send troops to Portland, Oregon, in latest deployment to US cities\n",
            "   Link: https://www.union-bulletin.com/news/national/ap-news-summary-at-3-17-p-m-edt/article_4b4a9e66-ff34-56ca-bbac-b6aca7dd57ec.html\n",
            "   Published: 2025-09-27 19:17:41\n",
            "   Source: union_bulletin\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsSentimentClassification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data from Step 1\n",
        "data = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\",),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally in Tamil Nadu\",),\n",
        "    (\"WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans\",),\n",
        "    (\"Beware of downtown Boise traffic congestion after BSU football game\",),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1\",),\n",
        "    (\"New Tourism Projects A Rs 15,000 cr Scam: Harish\",)\n",
        "]\n",
        "\n",
        "columns = [\"title\"]\n",
        "df = spark.createDataFrame(data, columns)\n"
      ],
      "metadata": {
        "id": "lPylURKeM6Iv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# 1. Clean text: lowercase and remove special characters\n",
        "df_clean = df.withColumn(\"clean_title\", lower(regexp_replace(col(\"title\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n",
        "\n",
        "# 2. Tokenize\n",
        "tokenizer = Tokenizer(inputCol=\"clean_title\", outputCol=\"words\")\n",
        "df_words = tokenizer.transform(df_clean)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df_filtered = remover.transform(df_words)\n",
        "\n",
        "# 4. TF-IDF features\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "df_featurized = hashingTF.transform(df_filtered)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idf_model = idf.fit(df_featurized)\n",
        "df_final = idf_model.transform(df_featurized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtnC51omM-5Y",
        "outputId": "cdccddae-4455-448e-f353-6acfd60e69fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3993661836.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  df_clean = df.withColumn(\"clean_title\", lower(regexp_replace(col(\"title\"), \"[^a-zA-Z0-9\\s]\", \"\")))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# 1. Clean text: lowercase and remove special characters\n",
        "df_clean = df.withColumn(\n",
        "    \"clean_title\",\n",
        "    lower(regexp_replace(col(\"title\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# 2. Tokenize the cleaned text\n",
        "tokenizer = Tokenizer(inputCol=\"clean_title\", outputCol=\"words\")\n",
        "df_words = tokenizer.transform(df_clean)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df_filtered = remover.transform(df_words)\n",
        "\n",
        "# 4. Convert words to TF features\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "df_featurized = hashingTF.transform(df_filtered)\n",
        "\n",
        "# 5. Apply IDF to get final features\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idf_model = idf.fit(df_featurized)\n",
        "df_final = idf_model.transform(df_featurized)\n",
        "\n",
        "# Show final dataframe\n",
        "df_final.select(\"title\", \"clean_title\", \"features\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRROnVhCNK_l",
        "outputId": "27ead2ac-f4d4-4686-b316-11d422318ab5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|title                                                                    |clean_title                                                            |features                                                                                                                                                                                                                                 |\n",
            "+-------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Mining pit collapses in Zamfara, many feared dead                        |mining pit collapses in zamfara many feared dead                       |(1000,[188,395,467,530,602,825,928],[1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368])                                                                     |\n",
            "|Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally in Tamil Nadu|karur stampede anatomy of the tragedy at vijays tvk rally in tamil nadu|(1000,[261,368,381,386,520,548,572,707,818],[1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368])                         |\n",
            "|WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans |watch shut the f up  rory mcilroy aims expletive at american fans      |(1000,[135,203,372,373,494,652,708,768,844,988],[1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,0.8472978603872037,1.252762968495368,1.252762968495368])  |\n",
            "|Beware of downtown Boise traffic congestion after BSU football game      |beware of downtown boise traffic congestion after bsu football game    |(1000,[6,103,104,484,828,892,894,975],[1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,0.8472978603872037])                                                |\n",
            "|Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1 |tigers ace tarik skubal prepares for a big start be it sunday or game 1|(1000,[48,130,534,752,768,812,931,970,975,996],[1.252762968495368,1.252762968495368,1.252762968495368,0.8472978603872037,0.8472978603872037,1.252762968495368,1.252762968495368,1.252762968495368,0.8472978603872037,0.8472978603872037])|\n",
            "|New Tourism Projects A Rs 15,000 cr Scam: Harish                         |new tourism projects a rs 15000 cr scam harish                         |(1000,[157,407,585,743,752,832,874,996],[1.252762968495368,1.252762968495368,1.252762968495368,1.252762968495368,0.8472978603872037,1.252762968495368,1.252762968495368,0.8472978603872037])                                             |\n",
            "+-------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize Spark\n",
        "# -----------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsSentimentClassification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------\n",
        "# Sample labeled dataset for training\n",
        "# 0 = Negative, 1 = Positive\n",
        "# (You can expand this with more labeled headlines)\n",
        "data = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\", 0),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally\", 0),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start\", 1),\n",
        "    (\"Beware of downtown Boise traffic congestion after BSU football game\", 0),\n",
        "    (\"New Tourism Projects A Rs 15,000 cr Scam: Harish\", 0),\n",
        "    (\"Rory McIlroy wins Ryder Cup match\", 1),\n",
        "    (\"Stock market hits record high\", 1),\n",
        "    (\"Severe weather alert issued in Texas\", 0)\n",
        "]\n",
        "\n",
        "df_labeled = spark.createDataFrame(data, [\"title\", \"label\"])\n",
        "\n",
        "# -----------------------------\n",
        "# Build ML Pipeline\n",
        "# -----------------------------\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
        "\n",
        "# -----------------------------\n",
        "# Train the model\n",
        "# -----------------------------\n",
        "model = pipeline.fit(df_labeled)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on new headlines (from Step 1)\n",
        "# -----------------------------\n",
        "new_headlines = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\",),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally\",),\n",
        "    (\"WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans\",),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1\",),\n",
        "    (\"New Tourism Projec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "_yQA8j_mNY_s",
        "outputId": "2fca3c86-51ec-4cd9-904c-4a2c838cf722"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 55) (ipython-input-3253776518.py, line 55)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3253776518.py\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    (\"New Tourism Projec\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize Spark\n",
        "# -----------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsSentimentClassification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------\n",
        "# Sample labeled dataset for training\n",
        "# 0 = Negative, 1 = Positive\n",
        "# (You can expand this with more labeled headlines)\n",
        "data = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\", 0),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally\", 0),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start\", 1),\n",
        "    (\"Beware of downtown Boise traffic congestion after BSU football game\", 0),\n",
        "    (\"New Tourism Projects A Rs 15,000 cr Scam: Harish\", 0),\n",
        "    (\"Rory McIlroy wins Ryder Cup match\", 1),\n",
        "    (\"Stock market hits record high\", 1),\n",
        "    (\"Severe weather alert issued in Texas\", 0)\n",
        "]\n",
        "\n",
        "df_labeled = spark.createDataFrame(data, [\"title\", \"label\"])\n",
        "\n",
        "# -----------------------------\n",
        "# Build ML Pipeline\n",
        "# -----------------------------\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
        "\n",
        "# -----------------------------\n",
        "# Train the model\n",
        "# -----------------------------\n",
        "model = pipeline.fit(df_labeled)\n",
        "\n",
        "# -----------------------------\n",
        "# Predict on new headlines (from Step 1)\n",
        "# -----------------------------\n",
        "new_headlines = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\",),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally\",),\n",
        "    (\"WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans\",),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1\",),\n",
        "    (\"New Tourism Projects A Rs 15,000 cr Scam: Harish\",)\n",
        "]\n",
        "\n",
        "df_new = spark.createDataFrame(new_headlines, [\"title\"])\n",
        "predictions = model.transform(df_new)\n",
        "\n",
        "# -----------------------------\n",
        "# Show results\n",
        "# -----------------------------\n",
        "predictions.select(\"title\", \"prediction\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM5ty8-TNcs7",
        "outputId": "2ba49c68-e56c-4a78-8ce4-ffc2986b7efc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------+----------+\n",
            "|title                                                                   |prediction|\n",
            "+------------------------------------------------------------------------+----------+\n",
            "|Mining pit collapses in Zamfara, many feared dead                       |0.0       |\n",
            "|Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally             |0.0       |\n",
            "|WATCH: 'Shut the f*** up' - Rory McIlroy aims expletive at American fans|1.0       |\n",
            "|Tigers ace Tarik Skubal prepares for a big start, be it Sunday or Game 1|1.0       |\n",
            "|New Tourism Projects A Rs 15,000 cr Scam: Harish                        |0.0       |\n",
            "+------------------------------------------------------------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"NewsSentimentClassification\").getOrCreate()\n",
        "\n",
        "# Sample labeled dataset\n",
        "data = [\n",
        "    (\"Mining pit collapses in Zamfara, many feared dead\", 0),\n",
        "    (\"Karur stampede: Anatomy of the tragedy at Vijay’s TVK rally\", 0),\n",
        "    (\"Tigers ace Tarik Skubal prepares for a big start\", 1),\n",
        "    (\"Beware of downtown Boise traffic congestion after BSU football game\", 0),\n",
        "    (\"New Tourism Projects A Rs 15,000 cr Scam: Harish\", 0)\n",
        "]\n",
        "\n",
        "df_labeled = spark.createDataFrame(data, [\"title\", \"label\"])\n",
        "\n",
        "# Build pipeline\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(df_labeled)\n",
        "\n",
        "# Save the model for later use\n",
        "model.write().overwrite().save(\"/content/news_sentiment_model\")"
      ],
      "metadata": {
        "id": "0q0U6KgfOFVo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c22d026f",
        "outputId": "112095d5-0573-40c7-96de-5eff43070701"
      },
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session if not already initialized\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"LoadNewsSentimentModel\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Load the saved pipeline model\n",
        "loaded_model = PipelineModel.load(\"/content/news_sentiment_model\")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "\n",
        "# Simulate streaming input from a folder of JSON files\n",
        "# Each JSON file: {\"title\": \"headline text\"}\n",
        "news_stream = spark.readStream.schema(\"title STRING\").json(dummy_stream_folder)\n",
        "\n",
        "# Preprocess headlines\n",
        "news_clean = news_stream.withColumn(\n",
        "    \"clean_title\", lower(regexp_replace(col(\"title\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# Apply the loaded model\n",
        "predictions = loaded_model.transform(news_clean)\n",
        "\n",
        "# Select relevant columns\n",
        "output = predictions.select(\"title\", \"prediction\")\n",
        "\n",
        "# Write output to console\n",
        "query = output.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .start()\n",
        "\n",
        "# Note: In a real streaming scenario, you would want to monitor the query.\n",
        "# For this example with static files, the query will terminate after processing files.\n",
        "# query.awaitTermination()"
      ],
      "metadata": {
        "id": "DrNivoQ0OlC_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22636b92",
        "outputId": "20e93a78-2ef6-4568-f454-19216ad1327f"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the dummy directory path\n",
        "dummy_stream_folder = \"/content/news_stream_folder\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(dummy_stream_folder):\n",
        "    os.makedirs(dummy_stream_folder)\n",
        "\n",
        "# Sample news headlines\n",
        "sample_headlines = [\n",
        "    {\"title\": \"Breaking news: Local team wins championship!\"},\n",
        "    {\"title\": \"Tragic accident reported on highway\"},\n",
        "    {\"title\": \"New technology breakthrough announced\"},\n",
        "    {\"title\": \"Market shows signs of recovery\"},\n",
        "    {\"title\": \"Political tensions rise in the region\"}\n",
        "]\n",
        "\n",
        "# Write sample headlines to JSON files in the dummy directory\n",
        "for i, headline in enumerate(sample_headlines):\n",
        "    file_path = os.path.join(dummy_stream_folder, f\"news_{i}.json\")\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(headline, f)\n",
        "\n",
        "print(f\"Dummy directory '{dummy_stream_folder}' created with sample JSON files.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy directory '/content/news_stream_folder' created with sample JSON files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51f7f41",
        "outputId": "12c31e2d-99a4-4860-e814-826de8f1d1af"
      },
      "source": [
        "%pip install streamlit"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize Spark\n",
        "# -----------------------------\n",
        "spark = SparkSession.builder.appName(\"NewsSentimentDashboard\").getOrCreate()\n",
        "\n",
        "# Load pre-trained model\n",
        "model = PipelineModel.load(\"/content/news_sentiment_model\")\n",
        "\n",
        "# -----------------------------\n",
        "# Streaming input (folder of JSON files)\n",
        "# -----------------------------\n",
        "news_stream = spark.readStream.schema(\"title STRING\").json(\"/content/news_stream_folder\")\n",
        "\n",
        "# Preprocess headlines\n",
        "news_clean = news_stream.withColumn(\n",
        "    \"clean_title\", lower(regexp_replace(col(\"title\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# Apply model\n",
        "predictions = model.transform(news_clean).select(\"title\", \"prediction\")\n",
        "\n",
        "# -----------------------------\n",
        "# Streamlit dashboard\n",
        "# -----------------------------\n",
        "st.title(\"Real-Time News Sentiment Dashboard\")\n",
        "\n",
        "# Function to convert Spark DF to Pandas for visualization\n",
        "def spark_to_pandas(sdf):\n",
        "    try:\n",
        "        return sdf.toPandas()\n",
        "    except:\n",
        "        return pd.DataFrame(columns=[\"title\", \"prediction\"])\n",
        "\n",
        "# Placeholder for dynamic updates\n",
        "placeholder = st.empty()\n",
        "\n",
        "# Stream loop\n",
        "query = predictions.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"news_predictions\") \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    # Read latest predictions from memory table\n",
        "    sdf = spark.sql(\"SELECT * FROM news_predictions\")\n",
        "    df = spark_to_pandas(sdf)\n",
        "\n",
        "    # Display counts of Positive / Negative\n",
        "    if not df.empty:\n",
        "        sentiment_counts = df[\"prediction\"].value_counts()\n",
        "        st.bar_chart(sentiment_counts)\n",
        "\n",
        "        # Show latest headlines with sentiment\n",
        "        df[\"Sentiment\"] = df[\"prediction\"].map({0: \"Negative\", 1: \"Positive\"})\n",
        "        st.dataframe(df[[\"title\", \"Sentiment\"]].tail(20))\n",
        "\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NrQq3oQPIIR",
        "outputId": "acc1aa86-1f8e-43ff-bbf2-9b0cd3ba46be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}
